import os
import json
import pickle
from pathlib import Path
from datetime import datetime

import faiss
import numpy as np
import pandas as pd
from langchain_text_splitters import RecursiveCharacterTextSplitter
from openai import OpenAI
from tqdm import tqdm

from time_logger import timed
from token_logger import token_logger
from config import RAG_CONFIG, KNOWLEDGE_BASE_BUILDER_CONFIG

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö (–∏–∑ config.py)
# –î–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ KNOWLEDGE_BASE_BUILDER_CONFIG (–∫–∞–∫ –≤ chunk_data.py)
CHUNK_SIZE = KNOWLEDGE_BASE_BUILDER_CONFIG["chunk_size"]  # 1500 (–∏–∑ chunk_data.py)
CHUNK_OVERLAP = KNOWLEDGE_BASE_BUILDER_CONFIG["chunk_overlap"]  # 200 (–∏–∑ chunk_data.py)
# –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ RAG_CONFIG
EMBEDDING_BATCH_SIZE = RAG_CONFIG["embedding_batch_size"]
FAISS_DIMENSION = RAG_CONFIG["faiss_dimension"]
K_FINAL_CHUNKS = RAG_CONFIG["k_final_chunks"]
USE_RERANKER = RAG_CONFIG["use_reranker"]
RETRIEVAL_K_FOR_RERANK = RAG_CONFIG["retrieval_k_for_rerank"]


class RegulatoryConsultant:
    def __init__(self,
                 open_router_client: OpenAI,
                 embedding_model: str,
                 generation_model: str,
                 use_local_files: bool,
                 save_local_files: bool,
                 regulatory_consultant_faiss_index_path: str,
                 regulatory_consultant_chunks_path: str
                 ):
        self.open_router_client = open_router_client
        self.embedding_model = embedding_model
        self.generation_model = generation_model
        self.use_local_files = use_local_files
        self.save_local_files = save_local_files
        self.faiss_index_path = regulatory_consultant_faiss_index_path
        self.chunks_path = regulatory_consultant_chunks_path
        self.faiss_index = None
        self.corpus_chunks = None
        self._create_rag_artefacts()

    @timed
    def _should_rebuild_knowledge_base(self, raw_documents_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤."""
        try:
            from knowledge_base_builder.build_raw_data import SOURCE_FILES
            
            # –ï—Å–ª–∏ raw_documents.jsonl –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω—É–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å
            if not os.path.exists(raw_documents_path):
                print(f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {raw_documents_path}. –¢—Ä–µ–±—É–µ—Ç—Å—è —Å–±–æ—Ä–∫–∞.")
                return True
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ raw_documents.jsonl
            raw_docs_mtime = os.path.getmtime(raw_documents_path)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã
            for file_info in SOURCE_FILES:
                source_path = file_info['path']
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º Path –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è os.path.exists
                source_path_str = str(source_path)
                if os.path.exists(source_path_str):
                    source_mtime = os.path.getmtime(source_path_str)
                    if source_mtime > raw_docs_mtime:
                        print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª '{source_path.name}' –∏–∑–º–µ–Ω–µ–Ω. –¢—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.")
                        return True
            
            return False
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
            import traceback
            traceback.print_exc()
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –Ω–µ –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º
            return False

    @timed
    def _build_knowledge_base(self):
        """–í—ã–∑—ã–≤–∞–µ—Ç –∫–æ–¥ –∏–∑ knowledge_base_builder –¥–ª—è —Å–±–æ—Ä–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π."""
        try:
            print("--- –ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ knowledge_base_builder ---")
            from knowledge_base_builder.build_raw_data import main as build_main
            build_main()
            print("--- –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —É—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω–∞ ---")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–∫–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
            import traceback
            traceback.print_exc()
            raise

    @timed
    def _create_rag_artefacts(self):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        from config import RAW_DOCUMENTS_PATH
        
        if self._should_rebuild_knowledge_base(RAW_DOCUMENTS_PATH):
            self._build_knowledge_base()
        
        if self.use_local_files and os.path.exists(self.faiss_index_path) and os.path.exists(self.chunks_path):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ raw_documents.jsonl –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
            raw_docs_mtime = os.path.getmtime(RAW_DOCUMENTS_PATH) if os.path.exists(RAW_DOCUMENTS_PATH) else 0
            faiss_mtime = os.path.getmtime(self.faiss_index_path) if os.path.exists(self.faiss_index_path) else 0
            
            if raw_docs_mtime > faiss_mtime:
                print(f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –æ–±–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤. –¢—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ RAG.")
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º
                if os.path.exists(self.faiss_index_path):
                    os.remove(self.faiss_index_path)
                if os.path.exists(self.chunks_path):
                    os.remove(self.chunks_path)
            else:
                print(
                    f"–ò—Å–ø–æ–ª—å–∑—É—é —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ RAG-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã. –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ '{self.faiss_index_path}' –∏ '{self.chunks_path}'..."
                )
                self.faiss_index = faiss.read_index(self.faiss_index_path)
                with open(self.chunks_path, 'rb') as f:
                    self.corpus_chunks = pickle.load(f)
                print("–ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã RAG —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
                return
        
        # –ï—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞, –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
        print("RAG-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –±—É–¥—É—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã —Å –Ω—É–ª—è.")
        faiss_index, corpus_chunks = self._generate_new_rag_artefacts(RAW_DOCUMENTS_PATH)
        self.faiss_index = faiss_index
        self.corpus_chunks = corpus_chunks

        if self.save_local_files:
            print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ FAISS –≤ —Ñ–∞–π–ª '{self.faiss_index_path}'...")
            faiss.write_index(faiss_index, self.faiss_index_path)

            print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –≤ —Ñ–∞–π–ª '{self.chunks_path}'...")
            with open(self.chunks_path, 'wb') as f:
                pickle.dump(corpus_chunks, f)

    @timed
    def _get_embeddings_in_batches(self, texts_list, model, batch_size, show_progress=False):
        """
        –ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤, –æ—Ç–ø—Ä–∞–≤–ª—è—è –∏—Ö –ø–∞–∫–µ—Ç–∞–º–∏ (–±–∞—Ç—á–∞–º–∏).
        –≠—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ, —á–µ–º –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –ø–æ –æ–¥–Ω–æ–º—É.
        """
        all_embeddings = []
        iterator = range(0, len(texts_list), batch_size)

        if show_progress:
            iterator = tqdm(iterator, desc="–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")

        for i in iterator:
            batch = texts_list[i:i + batch_size]
            try:
                response = self.open_router_client.embeddings.create(input=batch, model=model)
                embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(embeddings)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞ {i // batch_size}: {e}")
                all_embeddings.extend([[0.0] * FAISS_DIMENSION] * len(batch))

        return np.array(all_embeddings).astype('float32')

    @timed
    def _generate_new_rag_artefacts(self, file_path):
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ RAG:
        1. –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSONL —Ñ–∞–π–ª–∞.
        2. –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—è —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ chunk_data.py.
        3. –°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è (—ç–º–±–µ–¥–¥–∏–Ω–≥–∏) –¥–ª—è —á–∞–Ω–∫–æ–≤.
        4. –°–æ–∑–¥–∞–µ—Ç –∏ –Ω–∞–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å FAISS.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: –∏–Ω–¥–µ–∫—Å FAISS –∏ —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤.
        """
        print("–®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSONL...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é chunk_all_documents –∏–∑ chunk_data.py –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏
        try:
            import sys
            from pathlib import Path
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
            project_root = Path(__file__).parent
            if str(project_root) not in sys.path:
                sys.path.insert(0, str(project_root))
            
            from knowledge_base_builder.chunk_data import chunk_all_documents
            
            print("–®–∞–≥ 2: –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É—è chunk_data.py)...")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ chunk_data.py –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤
            chunks_df = chunk_all_documents()
            
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(chunks_df)} —á–∞–Ω–∫–æ–≤ –∏–∑ chunk_data.py")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ –≤ CSV –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ –¥—Ä—É–≥–∏—Ö —Ü–µ–ª–µ–π (–∫–∞–∫ –≤ chunk_data.py)
            from knowledge_base_builder.chunk_data import CHUNKED_DOCS_PATH
            chunks_df.to_csv(CHUNKED_DOCS_PATH, index=False, encoding='utf-8')
            print(f"–ß–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ CSV: {CHUNKED_DOCS_PATH}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–≥–ª–∞–≤–∞, —Å—Ç–∞—Ç—å—è)
            documents_metadata = {}
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        doc = json.loads(line)
                        doc_id = doc.get('doc_id', 'unknown')
                        documents_metadata[doc_id] = doc
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è RAG
            all_chunks = []
            for _, row in chunks_df.iterrows():
                chunk_id = row['chunk_id']
                chunk_text = row['chunk_text']
                source_name = row['source_name']
                source_type = row['source_type']
                title = row['original_doc_title']
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º doc_id –∏–∑ chunk_id (—Ñ–æ—Ä–º–∞—Ç: "doc_id_chunk_N")
                # –ù—É–∂–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏–∑–≤–ª–µ—á—å doc_id, —É—á–∏—Ç—ã–≤–∞—è —á—Ç–æ –æ–Ω –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è
                chunk_id_parts = chunk_id.split('_chunk_')
                if len(chunk_id_parts) > 0:
                    doc_id = chunk_id_parts[0]
                else:
                    doc_id = chunk_id
                
                # –ü–æ–ª—É—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                doc_metadata = documents_metadata.get(doc_id, {})
                metadata_info = doc_metadata.get('metadata', {})
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–µ—Ñ–∏–∫—Å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (–∫–∞–∫ –±—ã–ª–æ —Ä–∞–Ω—å—à–µ)
                metadata_prefix = f"–ò—Å—Ç–æ—á–Ω–∏–∫: {source_name} ({source_type}). "
                if title:
                    metadata_prefix += f"–ù–∞–∑–≤–∞–Ω–∏–µ: {title}. "
                if metadata_info:
                    chapter = metadata_info.get('chapter', '')
                    article = metadata_info.get('article_number', '')
                    if chapter:
                        metadata_prefix += f"–ì–ª–∞–≤–∞: {chapter}. "
                    if article:
                        metadata_prefix += f"–°—Ç–∞—Ç—å—è: {article}. "
                
                all_chunks.append(metadata_prefix + chunk_text)
            
            print(f"–í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ chunk_data.py: {e}")
            import traceback
            traceback.print_exc()
            print("–ò—Å–ø–æ–ª—å–∑—É–µ–º fallback: —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ –Ω–∞–ø—Ä—è–º—É—é...")
            # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–±, –µ—Å–ª–∏ chunk_data.py –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
            documents = []
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        documents.append(json.loads(line))
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=CHUNK_SIZE,
                chunk_overlap=CHUNK_OVERLAP,
                length_function=len,
                add_start_index=True
            )
            all_chunks = []
            
            for doc in documents:
                doc_id = doc.get('doc_id', 'unknown')
                source_name = doc.get('source_name', '')
                source_type = doc.get('source_type', '')
                title = doc.get('title', '')
                metadata_info = doc.get('metadata', {})
                
                metadata_prefix = f"–ò—Å—Ç–æ—á–Ω–∏–∫: {source_name} ({source_type}). "
                if title:
                    metadata_prefix += f"–ù–∞–∑–≤–∞–Ω–∏–µ: {title}. "
                if metadata_info:
                    chapter = metadata_info.get('chapter', '')
                    article = metadata_info.get('article_number', '')
                    if chapter:
                        metadata_prefix += f"–ì–ª–∞–≤–∞: {chapter}. "
                    if article:
                        metadata_prefix += f"–°—Ç–∞—Ç—å—è: {article}. "
                
                content = doc.get('content', '')
                if not content:
                    continue
                    
                chunks = text_splitter.split_text(content)
                for chunk in chunks:
                    all_chunks.append(metadata_prefix + chunk)
            
            print(f"–í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (fallback).")
        print(f"–®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —á–∞–Ω–∫–æ–≤ (–º–æ–¥–µ–ª—å: {self.embedding_model})...")
        chunk_embeddings = self._get_embeddings_in_batches(all_chunks, self.embedding_model, EMBEDDING_BATCH_SIZE,
                                                           show_progress=True)

        print("–®–∞–≥ 4: –°–æ–∑–¥–∞–Ω–∏–µ –∏ –Ω–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ FAISS...")
        index = faiss.IndexFlatL2(FAISS_DIMENSION)
        index.add(chunk_embeddings)
        print(f"–ò–Ω–¥–µ–∫—Å FAISS —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω. –í –Ω–µ–º {index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤.")

        return index, all_chunks

    @timed
    def _expand_question(self, question: str) -> list[str]:
        """–ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –≤–æ–ø—Ä–æ—Å–∞."""
        prompt = f"""–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å 3 –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞, —á—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å –ø–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –ù–µ –æ—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∞ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π –µ–≥–æ. –í—ã–≤–µ–¥–∏ –∫–∞–∂–¥—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏, –±–µ–∑ –Ω—É–º–µ—Ä–∞—Ü–∏–∏.
    
    –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å: {question}
    
    –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏:"""
        try:
            response = self.open_router_client.chat.completions.create(
                model=self.generation_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.8
            )
            expanded_queries = response.choices[0].message.content.strip().split('\n')
            token_logger.log_usage(response.usage, self.generation_model, "expand_question",
                                   f"{question=} {expanded_queries=}")
            return [q.strip() for q in expanded_queries if q.strip()]
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏ –≤–æ–ø—Ä–æ—Å–∞: {e}")
            return []

    @timed
    def _generate_hypothetical_answer(self, question: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å, –Ω–µ –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö,
        —á—Ç–æ–±—ã –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        prompt = f"""–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫–∏–π, –Ω–æ –ø–æ–ª–Ω—ã–π –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å. –≠—Ç–æ—Ç –æ—Ç–≤–µ—Ç –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –ù–µ –≥–æ–≤–æ—Ä–∏, —á—Ç–æ —Ç—ã –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞. –ü—Ä–æ—Å—Ç–æ –ø—Ä–∏–¥—É–º–∞–π –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç.
    
    –í–æ–ø—Ä–æ—Å: {question}
    
    –ì–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç:"""
        try:
            response = self.open_router_client.chat.completions.create(
                model=self.generation_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0
            )
            hypothetical_answer = response.choices[0].message.content
            token_logger.log_usage(response.usage, self.generation_model, "generate_hypothetical_answer",
                                   f"{question=} {hypothetical_answer=}")
            return hypothetical_answer
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç–≤–µ—Ç–∞: –Ω–∞ –≤–æ–ø—Ä–æ—Å {question}: {e}")
            return question

    @timed
    def answer_question(self, question):
        """
        –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –≤–æ–ø—Ä–æ—Å, –†–ê–°–®–ò–†–Ø–ï–¢ –µ–≥–æ, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å,
        –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç.
        """
        all_queries = [question]

        expanded_questions = self._expand_question(question)
        hypothetical_answer = self._generate_hypothetical_answer(question)

        all_queries.extend(expanded_questions)
        all_queries.append(hypothetical_answer)

        query_embeddings = self._get_embeddings_in_batches(all_queries, self.embedding_model, 10)

        retrieved_indices = set()
        k_retrieval = RETRIEVAL_K_FOR_RERANK if USE_RERANKER else K_FINAL_CHUNKS
        _, I = self.faiss_index.search(query_embeddings, k_retrieval)
        for indices_per_query in I:
            for idx in indices_per_query:
                retrieved_indices.add(idx)

        retrieved_chunks = [self.corpus_chunks[i] for i in retrieved_indices]

        # –£–±—Ä–∞–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ reranker
        final_chunks = retrieved_chunks
        context = "\n\n---\n\n".join(final_chunks)

        prompt = f"""–¢—ã ‚Äî —ç–º–ø–∞—Ç–∏—á–Ω—ã–π, –Ω–æ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π —ç–∫—Å–ø–µ—Ä—Ç. –¢–≤–æ—è –º–∏—Å—Å–∏—è ‚Äî –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∏–µ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ª–µ–∑–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Å–ª–æ–∂–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –¢–≤–æ–π —è–∑—ã–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º, –Ω–æ –∫—Ä–∏—Å—Ç–∞–ª—å–Ω–æ —è—Å–Ω—ã–º –¥–ª—è —á–µ–ª–æ–≤–µ–∫–∞ –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏.
    
    –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –ö–û–ù–¢–ï–ö–°–¢ –∏ –Ω–∞ –µ–≥–æ –æ—Å–Ω–æ–≤–µ –¥–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø.
    
    ### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
    
    –¢–≤–æ–π –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω —Å—Ç—Ä–æ–≥–æ —Å–ª–µ–¥–æ–≤–∞—Ç—å —ç—Ç–æ–π –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ:
    
    1.  **–í–≤–µ–¥–µ–Ω–∏–µ (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –Ω–æ –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ–º):**
        *   –ù–∞—á–Ω–∏ —Å –∫—Ä–∞—Ç–∫–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–æ–µ –æ–±–æ–∑–Ω–∞—á–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –≤–æ–ø—Ä–æ—Å–∞ –∏ –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏–Ω—Ü–∏–ø.
        *   *–ü—Ä–∏–º–µ—Ä: "–û—Ç–∑—ã–≤ –ª–∏—Ü–µ–Ω–∑–∏–∏ —É –±–∞–Ω–∫–∞ ‚Äî —Å—Ç—Ä–µ—Å—Å–æ–≤–∞—è —Å–∏—Ç—É–∞—Ü–∏—è, –Ω–æ –≤–∞—à–∏ —Å–±–µ—Ä–µ–∂–µ–Ω–∏—è –∑–∞—â–∏—â–µ–Ω—ã –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–æ–º. –ì–ª–∞–≤–Ω–æ–µ ‚Äî –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ. –í–æ—Ç –ø–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω:"*
    
    2.  **–ü—Ä—è–º–æ–π –∏ –µ–º–∫–∏–π –æ—Ç–≤–µ—Ç:**
        *   –°—Ä–∞–∑—É –¥–∞–π –≥–ª–∞–≤–Ω—ã–π –≤—ã–≤–æ–¥. **–í—ã–¥–µ–ª–∏ –µ–≥–æ –ø–æ–ª—É–∂–∏—Ä–Ω—ã–º.** –≠—Ç–æ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç–≤–µ—Ç –≤ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∏ —Å—Ä–∞–∑—É –ø–æ–Ω—è—Ç—å —Å—É—Ç—å.
        *   *–ü—Ä–∏–º–µ—Ä: "**–ü—Ä–æ—Å—Ä–æ—á–∫–∞ –ø–æ ¬´–±–µ—Å–ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–º—É¬ª –∑–∞–π–º—É –∞–Ω–Ω—É–ª–∏—Ä—É–µ—Ç –ª—å–≥–æ—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –∏ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ –Ω–∞—á–∏—Å–ª–µ–Ω–∏—é –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤, –ø–µ–Ω–µ–π –∏ —à—Ç—Ä–∞—Ñ–æ–≤ –∑–∞ –≤–µ—Å—å —Å—Ä–æ–∫, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç –∏—Ç–æ–≥–æ–≤—É—é –ø–µ—Ä–µ–ø–ª–∞—Ç—É –∏ –ø–æ–ª–Ω—É—é —Å—Ç–æ–∏–º–æ—Å—Ç—å –∫—Ä–µ–¥–∏—Ç–∞ (–ü–°–ö).**"*
    
    3.  **–î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ (–∏—Å–ø–æ–ª—å–∑—É–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ `### –î–µ—Ç–∞–ª–∏` –∏–ª–∏ `### –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç`):**
        *   –†–∞–∑–±–µ–π —Å–ª–æ–∂–Ω—ã–µ —Ç–µ–º—ã –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏ —Å **–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º–∏ –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ H4** (`#### 1. –ö–∞–∫ –ø—Ä–æ—Å—Ä–æ—á–∫–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø–µ—Ä–µ–ø–ª–∞—Ç—É`).
        *   –í–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–π **–º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏** (`*`) –¥–ª—è –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è –ø—Ä–∏—á–∏–Ω, —à–∞–≥–æ–≤, –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–π –∏–ª–∏ —Ñ–∞–∫—Ç–æ–≤.
        *   **–û–±—ä—è—Å–Ω—è–π —Å–ª–æ–∂–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã** (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ü–°–ö, –ê–°–í, –§–ó-353) —Å—Ä–∞–∑—É –ø—Ä–∏ –ø–µ—Ä–≤–æ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–∏, –º–æ–∂–Ω–æ –≤ —Å–∫–æ–±–∫–∞—Ö.
        *   –í–∫–ª—é—á–∞–π **–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã, —Å—Ä–æ–∫–∏ –∏ –ø—Ä–∏–º–µ—Ä—ã** –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–∞–≥–ª—è–¥–Ω—ã–º –∏ –¥–æ–∫–∞–∑—É–µ–º—ã–º.
        *   –°—Å—ã–ª–∞–π—Å—è –Ω–∞ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —á—Ç–æ–±—ã –ø–æ–¥–∫—Ä–µ–ø–∏—Ç—å –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "—Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ç. 1154 –ì–ö –†–§...").
    
    4.  **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã (–∏—Å–ø–æ–ª—å–∑—É–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ `### –ß—Ç–æ –¥–µ–ª–∞—Ç—å` –∏–ª–∏ `### –°–æ–≤–µ—Ç—ã`):**
        *   –ó–∞–≤–µ—Ä—à–∏ –æ—Ç–≤–µ—Ç –±–ª–æ–∫–æ–º —Å **–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏, –¥–µ–π—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —à–∞–≥–∞–º–∏**, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –ø—Ä–µ–¥–ø—Ä–∏–Ω—è—Ç—å.
        *   –û—Ñ–æ—Ä–º–ª—è–π —Å–æ–≤–µ—Ç—ã –≤ –≤–∏–¥–µ –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–ª–∏ –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞.
        *   *–ü—Ä–∏–º–µ—Ä: "- **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ–≥–æ–≤–æ—Ä:** –ù–∞–π–¥–∏—Ç–µ —Ä–∞–∑–¥–µ–ª—ã –æ —à—Ç—Ä–∞—Ñ–∞—Ö. - **–°–≤—è–∂–∏—Ç–µ—Å—å —Å –∫—Ä–µ–¥–∏—Ç–æ—Ä–æ–º:** –ü–æ–ø—ã—Ç–∞–π—Ç–µ—Å—å –¥–æ–≥–æ–≤–æ—Ä–∏—Ç—å—Å—è –æ —Ä–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏–∑–∞—Ü–∏–∏."*
    
    ### –°—Ç–∏–ª—å –∏ —Ç–æ–Ω: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —ç–º–æ–¥–∑–∏
    
    –ß—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –æ—Ç–≤–µ—Ç –±–æ–ª–µ–µ –∂–∏–≤—ã–º –∏ –ø–æ–Ω—è—Ç–Ω—ã–º, –∏—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ —É–º–µ—Å—Ç–Ω–æ –∏ –¥–æ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ. –û–Ω–∏ –¥–æ–ª–∂–Ω—ã —Å–ª—É–∂–∏—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ –∞–∫—Ü–µ–Ω—Ç–∞–º–∏ –∏ —É—Å–∏–ª–∏–≤–∞—Ç—å —ç–º–ø–∞—Ç–∏—é.
    
    *   **–ü—Ä–∞–≤–∏–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
        *   –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –ø—É–Ω–∫—Ç–æ–≤ –≤ —Å–ø–∏—Å–∫–∞—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Ä–∞–∑–¥–µ–ª–µ "–ß—Ç–æ –¥–µ–ª–∞—Ç—å".
        *   –†–∞–∑–º–µ—â–∞–π –∏—Ö –≤ –Ω–∞—á–∞–ª–µ –∏–ª–∏ –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∞–∫—Ü–µ–Ω—Ç–∞.
        *   –í—ã–±–∏—Ä–∞–π —ç–º–æ–¥–∑–∏, –∫–æ—Ç–æ—Ä—ã–µ –ª–æ–≥–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω—ã —Å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º.
    *   **–ß—Ç–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
        *   –î–ª—è —Å–æ–≤–µ—Ç–æ–≤ –∏ —à–∞–≥–æ–≤: ‚úÖ, ‚û°Ô∏è, ‚úçÔ∏è, üìû, üóìÔ∏è
        *   –î–ª—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –∏ –≤–∞–∂–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤: ‚ö†Ô∏è, ‚ùóÔ∏è, üí°
        *   –î–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ç–µ–º: üí∞, üìÑ, üìà, üè¶, üí≥
    *   **–ß–µ–≥–æ —Å–ª–µ–¥—É–µ—Ç –∏–∑–±–µ–≥–∞—Ç—å:**
        *   **–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏** –≤ –≥–ª–∞–≤–Ω–æ–º –≤—ã–≤–æ–¥–µ (–≤—ã–¥–µ–ª–µ–Ω–Ω–æ–º –ø–æ–ª—É–∂–∏—Ä–Ω—ã–º).
        *   –ò–∑–±–µ–≥–∞–π —á—Ä–µ–∑–º–µ—Ä–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–º–æ–¥–∑–∏ (–Ω–µ –±–æ–ª–µ–µ –æ–¥–Ω–æ–≥–æ –Ω–∞ –ø—É–Ω–∫—Ç —Å–ø–∏—Å–∫–∞ –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏–π –∞–±–∑–∞—Ü).
        *   –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –Ω–µ—É–º–µ—Å—Ç–Ω—ã–µ –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ —ç–º–æ–¥–∑–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, üòÇ, ü•≥, ü§Ø). –¢–æ–Ω –¥–æ–ª–∂–µ–Ω –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º.
    
    ### –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã, –∫–æ—Ç–æ—Ä—ã–º –Ω—É–∂–Ω–æ —Å–ª–µ–¥–æ–≤–∞—Ç—å
    
    *   **100% –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:** –¢–≤–æ–π –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –ö–û–ù–¢–ï–ö–°–¢–ï. –ù–µ –¥–æ–±–∞–≤–ª—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Å–≤–æ–∏—Ö –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∞ –∫–∞–∂–µ—Ç—Å—è –≤–µ—Ä–Ω–æ–π. –ö–∞–∂–¥–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
    *   **–ò—Å—á–µ—Ä–ø—ã–≤–∞—é—â–µ, –Ω–æ –±–µ–∑ "–≤–æ–¥—ã":** –ò—Å–ø–æ–ª—å–∑—É–π –í–°–ï —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –°–∏–Ω—Ç–µ–∑–∏—Ä—É–π –∏—Ö –≤ –ª–æ–≥–∏—á–Ω—ã–π —Ä–∞—Å—Å–∫–∞–∑. –ù–µ —É–ø—É—Å–∫–∞–π –¥–µ—Ç–∞–ª–∏, –Ω–æ –∏–∑–±–µ–≥–∞–π –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π.
    *   **–ù–∏–∫–∞–∫–∏—Ö —Å–∞–º–æ—Å—Å—ã–ª–æ–∫:** –ù–∏–∫–æ–≥–¥–∞ –Ω–µ —É–ø–æ–º–∏–Ω–∞–π "–∫–æ–Ω—Ç–µ–∫—Å—Ç", "–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é" –∏–ª–∏ "–±–∞–∑—É –∑–Ω–∞–Ω–∏–π" –≤ —Å–≤–æ–µ–º –æ—Ç–≤–µ—Ç–µ. –ì–æ–≤–æ—Ä–∏ –æ—Ç –ª–∏—Ü–∞ —ç–∫—Å–ø–µ—Ä—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –≤–ª–∞–¥–µ–µ—Ç —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π.
    *   **–û—Ç–∫–∞–∑ –æ—Ç –æ—Ç–≤–µ—Ç–∞:** –ï—Å–ª–∏ –≤ –ö–û–ù–¢–ï–ö–°–¢–ï –∞–±—Å–æ–ª—é—Ç–Ω–æ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å, –Ω–∞–ø–∏—à–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É —Ñ—Ä–∞–∑—É: `–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤ –º–æ–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É.`
    
    ---
    
    ### –ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô
    {context}
    
    ### –í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø
    {question}
    
    ### –¢–í–û–ô –û–¢–í–ï–¢
    """
        try:
            response = self.open_router_client.chat.completions.create(
                model=self.generation_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0
            )
            final_answer = response.choices[0].message.content
            token_logger.log_usage(response.usage, self.generation_model, "answer_question",
                                   f"{question=} {final_answer=}")
            return final_answer
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å '{question}': {e}")
            return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."
